{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "\n",
    "prostate_whole_dataset = dataset.ProstatDS(\"dataset/prostate.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, val, train = dataset.dataset_split(prostate_whole_dataset, [0.2, 0.1])\n",
    "train.normalize()\n",
    "val.normalize(train.norm_values)\n",
    "_ = test.normalize(train.norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with reg=0.00e+00:\n",
      "W1, error=7.5e-07\n",
      "b1, error=2.2e-06\n",
      "W2, error=1.9e-07\n",
      "b2, error=7.5e-08\n",
      "Testing with reg=1.00e-01:\n",
      "W1, error=4.9e-06\n",
      "b1, error=4.7e-05\n",
      "W2, error=5.2e-07\n",
      "b2, error=8.4e-07\n"
     ]
    }
   ],
   "source": [
    "from gradient_check import eval_numeric_gradient\n",
    "from gradient_check import rel_error\n",
    "import models\n",
    "\n",
    "sample, _ = dataset.dataset_split(train, 40.0 / train.num_samples)\n",
    "X = sample.X.astype(\"float64\")\n",
    "\n",
    "for reg in [0, 0.1]:\n",
    "  model = models.TwoLayerNet(train.num_features, 20, reg=reg, dtype=np.float64)\n",
    "  \n",
    "  # Post-initialization: Make the 'b' params random values as well:\n",
    "  for key in model.params.keys():\n",
    "    p = model.params[key]\n",
    "    if key.startswith('b'):\n",
    "      model.params[key] = 0.01 * np.random.randn(*p.shape)\n",
    "\n",
    "  print(\"Testing with reg=%.2e:\" % reg)\n",
    "\n",
    "  for key in model.params.keys():\n",
    "    f = lambda : model.loss(X, sample.y)[0]\n",
    "    grad_n = eval_numeric_gradient(f, model.params[key])\n",
    "    grad_a = model.loss(sample.X, sample.y)[1][key]\n",
    "    error = rel_error(grad_n, grad_a)\n",
    "    print(\"%s, error=%.1e\" % (key, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model):\n",
    "  y_train = model.loss(train.X)\n",
    "  acc_train = np.mean(y_train == train.y)\n",
    "  y_val = model.loss(val.X)\n",
    "  acc_val = np.mean(y_val == val.y)\n",
    "  print(\"Accuracy: Training = %.1f%%, Validation = %.1f%%\" % (acc_train * 100, acc_val * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0]: loss=0.858260\n",
      "Accuracy: Training = 61.1%, Validation = 48.0%\n",
      "[  10]: loss=0.546587\n",
      "Accuracy: Training = 69.6%, Validation = 62.0%\n",
      "[  20]: loss=0.568961\n",
      "Accuracy: Training = 65.9%, Validation = 52.0%\n",
      "[  30]: loss=0.545746\n",
      "Accuracy: Training = 68.2%, Validation = 54.0%\n",
      "[  40]: loss=0.526709\n",
      "Accuracy: Training = 71.3%, Validation = 54.0%\n",
      "[  50]: loss=0.518392\n",
      "Accuracy: Training = 71.9%, Validation = 54.0%\n",
      "[  60]: loss=0.478912\n",
      "Accuracy: Training = 73.9%, Validation = 58.0%\n",
      "[  70]: loss=0.452841\n",
      "Accuracy: Training = 77.3%, Validation = 60.0%\n",
      "[  80]: loss=0.418179\n",
      "Accuracy: Training = 79.5%, Validation = 60.0%\n",
      "[  90]: loss=0.383565\n",
      "Accuracy: Training = 81.0%, Validation = 58.0%\n",
      "[ 100]: loss=0.370580\n",
      "Accuracy: Training = 81.5%, Validation = 58.0%\n",
      "[ 110]: loss=0.386832\n",
      "Accuracy: Training = 81.8%, Validation = 58.0%\n",
      "[ 120]: loss=0.338641\n",
      "Accuracy: Training = 82.1%, Validation = 60.0%\n",
      "[ 130]: loss=0.342774\n",
      "Accuracy: Training = 84.7%, Validation = 64.0%\n",
      "[ 140]: loss=0.338646\n",
      "Accuracy: Training = 82.4%, Validation = 56.0%\n",
      "[ 150]: loss=0.263961\n",
      "Accuracy: Training = 89.2%, Validation = 66.0%\n",
      "[ 160]: loss=0.439002\n",
      "Accuracy: Training = 77.3%, Validation = 54.0%\n",
      "[ 170]: loss=0.232676\n",
      "Accuracy: Training = 92.0%, Validation = 68.0%\n",
      "[ 180]: loss=0.297124\n",
      "Accuracy: Training = 85.5%, Validation = 60.0%\n",
      "[ 190]: loss=0.245232\n",
      "Accuracy: Training = 91.8%, Validation = 62.0%\n",
      "[ 200]: loss=0.218589\n",
      "Accuracy: Training = 91.8%, Validation = 62.0%\n",
      "[ 210]: loss=0.488804\n",
      "Accuracy: Training = 85.8%, Validation = 58.0%\n",
      "[ 220]: loss=0.182312\n",
      "Accuracy: Training = 94.9%, Validation = 66.0%\n",
      "[ 230]: loss=0.204871\n",
      "Accuracy: Training = 92.6%, Validation = 58.0%\n",
      "[ 240]: loss=0.182273\n",
      "Accuracy: Training = 93.8%, Validation = 58.0%\n",
      "[ 250]: loss=0.173577\n",
      "Accuracy: Training = 93.8%, Validation = 60.0%\n",
      "[ 260]: loss=0.200971\n",
      "Accuracy: Training = 90.9%, Validation = 60.0%\n",
      "[ 270]: loss=0.186532\n",
      "Accuracy: Training = 94.9%, Validation = 62.0%\n",
      "[ 280]: loss=0.144316\n",
      "Accuracy: Training = 96.3%, Validation = 64.0%\n",
      "[ 290]: loss=0.134498\n",
      "Accuracy: Training = 96.3%, Validation = 62.0%\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "\n",
    "# H = 12\n",
    "reg = 1e-5\n",
    "learning_rate = 3\n",
    "learning_rate_decay = 0.99\n",
    "# model = models.TwoLayerNet(train.num_features, H, reg=reg)\n",
    "model = models.fc_multilayer_net([sample.num_features, 15, 2], reg=reg * 2)\n",
    "\n",
    "# HACK: set the model2 parameters to refer to model parameters:\n",
    "#model2.layers[0].W = model.params['W1']\n",
    "#model2.layers[0].b = model.params['b1']\n",
    "#model2.layers[2].W = model.params['W2']\n",
    "#model2.layers[2].b = model.params['b2']\n",
    "\n",
    "for i in range(300):\n",
    "  loss, grad = model.loss(train.X, train.y)\n",
    "  loss2, grad2 = model2.loss(train.X, train.y)\n",
    "\n",
    "  for key in grad.keys():\n",
    "    model.params[key] -= learning_rate * grad[key]\n",
    "  if i % 10 == 0:\n",
    "    learning_rate *= learning_rate_decay\n",
    "  if i % 10 == 0:\n",
    "    print(\"[%4d]: loss=%f\" % (i, loss))\n",
    "    accuracy(model)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.54%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.loss(test.X)\n",
    "print(\"Test Accuracy: %.2f%%\" % (np.mean(y_pred == test.y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Trying a moduler Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Affine(1)[15, 10]:float64, Relu, Affine(2)[10, 1]:float64]\n"
     ]
    }
   ],
   "source": [
    "from models import fc_multilayer_net_cl\n",
    "\n",
    "# Gradient-check the layers we've created:\n",
    "sample, _ = dataset.dataset_split(train, 40.0 / train.num_samples)\n",
    "X = sample.X.astype(\"float64\")\n",
    "\n",
    "H = 10\n",
    "model = fc_multilayer_net_cl([sample.num_features, 10, 1], reg=0, dtype=\"float64\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
